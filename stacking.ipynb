{"cells":[{"cell_type":"markdown","metadata":{"id":"9s_l4KgFKpw0"},"source":["# 검증 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kdo0rJjnKpw1","outputId":"81665fbd-c404-4ba2-db7c-42dfe1ed53e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.8921, F1-score: 0.2453\n"]}],"source":["\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from imblearn.ensemble import BalancedRandomForestClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from xgboost import XGBClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","splits = 5\n","kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n","\n","train = pd.read_csv('./data/train_prep.csv')\n","test = pd.read_csv('./data/test_prep.csv')\n","X_train = train.drop('target',axis=1)\n","y_train = train['target']\n","\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('knn', KNeighborsClassifier(n_neighbors = 1))\n","])\n","\n","base_models = [\n","    RandomForestClassifier(class_weight= 'balanced_subsample',max_depth = 12,min_samples_leaf= 2, min_samples_split= 6, n_estimators =  150,random_state=42),\n","    XGBClassifier(eval_metric='logloss',colsample_bytree =0.8, gamma= 0.2, learning_rate =0.05, max_depth =6, min_child_weight= 1, n_estimators=250, scale_pos_weight=10, subsample= 0.9, random_state=42),\n","    pipeline,\n","    LogisticRegression(class_weight={1:9}, max_iter=1000)\n","]\n","\n","# 메타 모델\n","meta_model = LogisticRegression(class_weight={1:9}, max_iter=1000)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n","\n","X_train.reset_index(drop=True,inplace=True)\n","X_test.reset_index(drop=True,inplace=True)\n","y_train.reset_index(drop=True,inplace=True)\n","y_test.reset_index(drop=True,inplace=True)\n","\n","# 베이스 모델들에 대한 예측값을 저장할 배열\n","X_train_meta = np.zeros((len(X_train), len(base_models)))\n","X_test_meta = np.zeros((splits, len(X_test), len(base_models)))\n","\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train,y_train)):\n","    train_x, valid_x = X_train.loc[train_idx], X_train.loc[valid_idx]\n","    train_y, valid_y = y_train[train_idx], y_train[valid_idx]\n","\n","    for i, model in enumerate(base_models):\n","        # 베이스 모델 학습\n","        model.fit(train_x, train_y)\n","\n","        # 테스트 데이터에 대한 예측값 저장\n","        X_train_meta[valid_idx, i] = model.predict_proba(valid_x)[:,1]\n","        X_test_meta[fold,:, i] = model.predict_proba(X_test)[:,1]\n","\n","X_test_meta = X_test_meta.mean(axis=0)\n","\n","# 메타 모델 학습\n","meta_model.fit(X_train_meta, y_train)\n","\n","final_predictions = meta_model.predict(X_test_meta)\n","\n","\n","from sklearn.metrics import make_scorer, f1_score\n","from sklearn.metrics import accuracy_score\n","\n","accuracy = accuracy_score(y_test, final_predictions)\n","f1 = f1_score(y_test, final_predictions)\n","print(f\"Accuracy: {accuracy:.4f}, F1-score: {f1:.4f}\")"]},{"cell_type":"markdown","metadata":{"id":"210YF2NsKpw3"},"source":["# 예측 코드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WNPmK7SQKpw3"},"outputs":[],"source":["splits = 5\n","kf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\n","\n","train = pd.read_csv('./data/train_prep.csv')\n","X_test = pd.read_csv('./data/test_prep.csv')\n","X_train = train.drop('target',axis=1)\n","y_train = train['target']\n","\n","pipeline = Pipeline([\n","    ('scaler', StandardScaler()),\n","    ('knn', KNeighborsClassifier(n_neighbors = 1))\n","])\n","\n","base_models = [\n","    RandomForestClassifier(class_weight= 'balanced_subsample',max_depth = 12,min_samples_leaf= 2, min_samples_split= 6, n_estimators =  150,random_state=42),\n","    XGBClassifier(eval_metric='logloss',colsample_bytree =0.8, gamma= 0.2, learning_rate =0.05, max_depth =6, min_child_weight= 1, n_estimators=250, scale_pos_weight=10, subsample= 0.9, random_state=42),\n","    pipeline\n","]\n","\n","# 메타 모델\n","meta_model = LogisticRegression(class_weight={1:9}, max_iter=1000)\n","\n","\n","X_train.reset_index(drop=True,inplace=True)\n","X_test.reset_index(drop=True,inplace=True)\n","y_train.reset_index(drop=True,inplace=True)\n","\n","# 베이스 모델들에 대한 예측값을 저장할 배열\n","X_train_meta = np.zeros((len(X_train), len(base_models)))\n","X_test_meta = np.zeros((splits, len(X_test), len(base_models)))\n","\n","for fold, (train_idx, valid_idx) in enumerate(kf.split(X_train,y_train)):\n","    train_x, valid_x = X_train.loc[train_idx], X_train.loc[valid_idx]\n","    train_y, valid_y = y_train[train_idx], y_train[valid_idx]\n","\n","    for i, model in enumerate(base_models):\n","        # 베이스 모델 학습\n","        model.fit(train_x, train_y)\n","\n","        # 테스트 데이터에 대한 예측값 저장\n","        X_train_meta[valid_idx, i] = model.predict_proba(valid_x)[:,1]\n","        X_test_meta[fold,:, i] = model.predict_proba(X_test)[:,1]\n","\n","X_test_meta = X_test_meta.mean(axis=0)\n","\n","# 메타 모델 학습\n","meta_model.fit(X_train_meta, y_train)\n","\n","final_predictions = meta_model.predict(X_test_meta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TZwGtQoKpw4"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}